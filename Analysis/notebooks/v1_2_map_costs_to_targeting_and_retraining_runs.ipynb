{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "916b9a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /Users/karolinegriesbach/Documents/Innkeepr/Git/consumption-based-costs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de372acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from src.utils.accounts import sanitize_account_name\n",
    "from src.utils.innkeepr_api import call_api_with_service_token, send_to_innkeepr_api_paginated\n",
    "from src.utils.constants import return_api_url_innkeepr\n",
    "from src.utils.github_connection import read_yaml_from_github\n",
    "from src.utils.databricks_pp import handle_databricks_cost\n",
    "from src.utils.azure_pp import handle_azure_costs\n",
    "from src.utils.aws_pp import handle_aws_costs\n",
    "from src.utils.stackit_pp import stack_pp\n",
    "from src.utils.cost_handling import return_cost_per_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1efafa",
   "metadata": {},
   "source": [
    "# Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "407901b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from_date = \"2024-01-01\"\n",
    "to_date=\"2025-06-23\"\n",
    "timestamp = \"2025-06-23 13:23:56.520433\"\n",
    "path_to_dir = f\"/Users/karolinegriesbach/Documents/Innkeepr/Git/consumption-based-costs/data/{from_date}_to_{to_date}/{timestamp}/\"\n",
    "path_to_data=f\"{path_to_dir}final_costs_with_azure_and_aws_and_db_{from_date}_{to_date}.csv\"\n",
    "path_to_save = f\"/Users/karolinegriesbach/Documents/Innkeepr/Git/consumption-based-costs/data/{from_date}_to_{to_date}/targeting_and_retraining/\"\n",
    "url = return_api_url_innkeepr()\n",
    "stackit_cost_handling = {\n",
    "    # \"start\": \"2024-11-11\",\n",
    "    \"exlude_date_ranges\": [\n",
    "        {\n",
    "            \"start\": \"2024-12-22\",\n",
    "            \"end\" : \"2025-01-03\",\n",
    "        }\n",
    "    ],\n",
    "    \"start\":\"2025-02-09\" # davor keine eindeutige Kostenzuordnung zw. azure und stackit möglich in den Zeitspannen: 12.11.24 - 19.11.24, 20.12.24-21.12.24, 04.01.25-06.01.25, 10.01.25-12.01.25, 07.02.25 - 08.02.25\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b4b0c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(path_to_save, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cb359e",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "303142c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orignal = pd.read_csv(path_to_data)\n",
    "df_orignal = df_orignal[[col for col in df_orignal.columns if \"Unnamed\" not in col]]\n",
    "df_orignal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc3325a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = stack_pp(df_orignal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97355723",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76ef5fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Prefect_Deployments\"] = df[\"Prefect_Deployments\"].replace(\"retrainng\", \"retraining\")\n",
    "df[\"Prefect_Deployments\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54048c17",
   "metadata": {},
   "source": [
    "# Filter Data\n",
    "Filter data for targeting runs only using Deployments and Prefect_Deployments. Valid strings are:\n",
    "\n",
    "- targeting\n",
    "- retraining\n",
    "- googleConversionUpdate (smart bidding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4ad10bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stackIT costs via Prefect_Deployments\n",
    "targeting_and_retraining_runs = df[(df[\"Deployments\"]==\"targeting\")|(df[\"Prefect_Deployments\"]==\"targeting\")|(df[\"Deployments\"]==\"retraining\")|(df[\"Prefect_Deployments\"]==\"retraining\")|(df[\"Deployments\"]==\"googleConversionUpdate\")|(df[\"Prefect_Deployments\"]==\"googleConversionUpdate\")]\n",
    "# Bug in cost extractor: \"retrainng\" instead of \"retraining\" (behoben)\n",
    "targeting_and_retraining_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35bfef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "targeting_and_retraining_runs[\"Prefect_Deployments\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be1098d",
   "metadata": {},
   "source": [
    "# Historical count of targeting runs and prefect runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9a8e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "targeting_runs = targeting_and_retraining_runs[targeting_and_retraining_runs[\"Prefect_Deployments\"]==\"targeting\"]\n",
    "retraining_runs = targeting_and_retraining_runs[targeting_and_retraining_runs[\"Prefect_Deployments\"]==\"retraining\"]\n",
    "runs_vc_by_date = targeting_runs.groupby(\"date\")[\"Prefect_Deployments\"].value_counts()\n",
    "runs_vc_by_date = pd.DataFrame(runs_vc_by_date).reset_index().rename(columns={\"count\":\"count targeting runs\"})\n",
    "runs_vc_by_date_retraining = retraining_runs.groupby(\"date\")[\"Prefect_Deployments\"].value_counts()\n",
    "runs_vc_by_date_retraining = pd.DataFrame(runs_vc_by_date_retraining).reset_index().rename(columns={\"count\":\"count retraining runs\"})\n",
    "audiences_unique_by_date = targeting_runs.groupby(\"date\")[\"audience_id\"].nunique()\n",
    "audiences_unique_by_date = pd.DataFrame(audiences_unique_by_date).reset_index()\n",
    "concat = pd.merge(runs_vc_by_date, audiences_unique_by_date, on=\"date\")\n",
    "fig = plt.figure(figsize=(20,8))\n",
    "sns.lineplot(\n",
    "    x=\"date\",\n",
    "    y=\"count targeting runs\",\n",
    "    data=runs_vc_by_date[runs_vc_by_date[\"date\"] > \"2025-01-01\"],\n",
    "    marker=\"o\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"# targeting runs\"\n",
    ")\n",
    "sns.lineplot(\n",
    "    x=\"date\",\n",
    "    y=\"count retraining runs\",\n",
    "    data=runs_vc_by_date_retraining[runs_vc_by_date_retraining[\"date\"] > \"2025-01-01\"],\n",
    "    marker=\"o\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"# retraining runs\"\n",
    ")\n",
    "sns.lineplot(\n",
    "    x=\"date\",\n",
    "    y=\"audience_id\",\n",
    "    data=audiences_unique_by_date[audiences_unique_by_date[\"date\"] > \"2025-01-01\"],\n",
    "    marker=\"o\",\n",
    "    linestyle=\":\",\n",
    "    label=\"unique audiences\"\n",
    ")\n",
    "plt.title(\"Count Daily targeting runs\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f02ba5",
   "metadata": {},
   "source": [
    "# Check Data Completion for node types, date and costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e23d4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(targeting_and_retraining_runs))\n",
    "null_values = targeting_and_retraining_runs[[\"node_name\",\"date\",\"charge\",\"machine.type\",\"audience_id\",\"duration\",\"total_charge_of_serviceName\",\"audience\"]].isnull().sum()\n",
    "null_values = pd.DataFrame(null_values).rename(columns={0:\"isnull\"})\n",
    "null_values[\"percentage_of_isnull\"] = null_values[\"isnull\"]/len(targeting_and_retraining_runs) * 100\n",
    "null_values.sort_values(by=\"isnull\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5695777",
   "metadata": {},
   "source": [
    "## Handling Missing Data\n",
    "- fehlend node_names mit existing node_names anreichern via targeting and retraining audience\n",
    "- fehlende Kosten über node_names und runtime anreichern\n",
    "- charges per targeting run erst ab dm 26.05.2025 via prefect_logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f05d04",
   "metadata": {},
   "source": [
    "## Query models for all active accounts\n",
    "All past models are queried for the existing tenants in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "926076e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query models to map audience node_name with targetingOutlook (is one of the main parameter to choose the node size)\n",
    "try:\n",
    "    models = pd.read_csv(f\"{path_to_save}all_models.csv\")\n",
    "    with open(f\"{path_to_save}ignore_tenants.json\", \"r\") as f:\n",
    "        ignore_tenants = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(\"query data\")\n",
    "    models=pd.DataFrame()\n",
    "    #min_date = targeting_and_retraining_runs[\"date\"].min()\n",
    "    #min_date = (pd.to_datetime(min_date)-timedelta(days=60)).strftime(\"%Y-%m-%d\")\n",
    "    ignore_tenants = []\n",
    "    accounts = call_api_with_service_token(f\"{url}/core/accounts/query\", {}, logging)\n",
    "    for tenant in targeting_and_retraining_runs[\"tenant\"].unique():\n",
    "        print(tenant)\n",
    "        account_id = [acc[\"id\"] for acc in accounts if sanitize_account_name(acc[\"name\"])==tenant]\n",
    "        if len(account_id) > 1:\n",
    "            raise Exception(f\"More than one account with name {tenant}\")\n",
    "        if len(account_id) == 0:\n",
    "            print(f\"Tenant {tenant} not found in accounts\")\n",
    "            ignore_tenants.append(tenant)\n",
    "            continue\n",
    "        account_id = account_id[0]\n",
    "        temp_models = send_to_innkeepr_api_paginated(\n",
    "            f\"{url}/models/query\",\n",
    "            account_id,\n",
    "            {},\n",
    "            logging\n",
    "        )\n",
    "        temp_models = pd.json_normalize(temp_models)\n",
    "        if len(temp_models) == 0:\n",
    "            print(f\"No models found for tenant {tenant}\")\n",
    "            ignore_tenants.append(tenant)\n",
    "            continue\n",
    "        #temp_models = temp_models[temp_models[\"created\"]>=min_date]\n",
    "        models = pd.concat([models, temp_models])\n",
    "    print(models.shape)\n",
    "    missing_audiences = targeting_runs[targeting_runs[\"tenant\"].isin(ignore_tenants)==False]\n",
    "    missing_audiences = missing_audiences[missing_audiences[\"audience_id\"].isin(models[\"audience\"].unique())==False]\n",
    "    if missing_audiences.empty == False:\n",
    "        print(f\"Missing {len(missing_audiences)} models\")\n",
    "        print(missing_audiences[[\"tenant\",\"audience_id\"]].drop_duplicates())\n",
    "    models.to_csv(f\"{path_to_save}all_models.csv\")\n",
    "    with open(f\"{path_to_save}ignore_tenants.json\", \"w\") as f:\n",
    "        json.dump(list(ignore_tenants), f)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d759b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "audiences_in_models_but_not_in_targeting_runs = models[\n",
    "    (models[\"audience\"].isin(targeting_runs[targeting_runs[\"tenant\"].isin(ignore_tenants)==False][\"audience_id\"].unique())==False) &\n",
    "    (models[\"created\"]>=targeting_runs[\"date\"].min())\n",
    "    ]\n",
    "if audiences_in_models_but_not_in_targeting_runs.empty == False:\n",
    "    print(f\"Found {len(audiences_in_models_but_not_in_targeting_runs)} models that are not in the targeting runs\")\n",
    "    raise Exception(audiences_in_models_but_not_in_targeting_runs[[\"audience\",\"path\",\"created\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d62248",
   "metadata": {},
   "source": [
    "## Merge Models and Targeting Runs by considering date and targetingOutlookDays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4297c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = models[[\"audience\",\"created\",\"targetingOutlookDays\"]]\n",
    "models.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cab7ffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "targeting_and_retraining_runs[\"tenant\"] = np.where(\n",
    "    targeting_and_retraining_runs[\"tenant\"].isnull(),\n",
    "    targeting_and_retraining_runs[\"account\"].str.replace(\" \",\"\").str.lower(),\n",
    "    targeting_and_retraining_runs[\"tenant\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cbc6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = models.rename(columns={\"audience\":\"audience_id\",\"created\":\"created_data_model_for_testing\"})\n",
    "# Filter data_model_for_testing to get the most recent model for each audience before the date in data_for_testing\n",
    "print(f\"targeting_and_retraining_runs = {len(targeting_and_retraining_runs)}\")\n",
    "merged_targeting_runs_with_models = pd.merge(targeting_and_retraining_runs, models, on=['audience_id'], how=\"left\")#suffixes=('_data_for_testing', '_data_model_for_testing'))\n",
    "# macht data where the model.created < data.timestamp\n",
    "merged_targeting_runs_with_models_with_previous_models = merged_targeting_runs_with_models[pd.to_datetime(merged_targeting_runs_with_models['created_data_model_for_testing'],utc=True) <= pd.to_datetime(merged_targeting_runs_with_models['timestamp'],utc=True)]\n",
    "merged_targeting_runs_with_models_with_previous_models = merged_targeting_runs_with_models_with_previous_models.sort_values(by='created_data_model_for_testing', ascending=False).drop_duplicates(subset=['Prefect_Deployments','tenant','audience_id','timestamp','node_name'], keep='first')\n",
    "print(f\"merged_targeting_runs_with_models_with_previous_models = {len(merged_targeting_runs_with_models_with_previous_models)}\")\n",
    "# get data where data.timestamp does not have a matching model\n",
    "merged_targeting_runs_with_models_without_previous_models = models.groupby(\"audience_id\")[\"created_data_model_for_testing\"].min()\n",
    "merged_targeting_runs_with_models_without_previous_models = pd.DataFrame(merged_targeting_runs_with_models_without_previous_models).reset_index()\n",
    "merged_targeting_runs_with_models_without_previous_models = pd.merge(targeting_and_retraining_runs, merged_targeting_runs_with_models_without_previous_models, on=\"audience_id\")\n",
    "merged_targeting_runs_with_models_without_previous_models = merged_targeting_runs_with_models_without_previous_models[pd.to_datetime(merged_targeting_runs_with_models_without_previous_models['timestamp'],utc=True) <= pd.to_datetime(merged_targeting_runs_with_models_without_previous_models['created_data_model_for_testing'],utc=True)]\n",
    "print(f\"merged_targeting_runs_with_models_without_previous_models = {len(merged_targeting_runs_with_models_without_previous_models)}\")\n",
    "# get data with null models\n",
    "considered_audiences = merged_targeting_runs_with_models_with_previous_models[\"audience_id\"].unique().tolist() + merged_targeting_runs_with_models_without_previous_models[\"audience_id\"].unique().tolist()\n",
    "merged_targeting_runs_with_models_null_models = merged_targeting_runs_with_models[\n",
    "    (merged_targeting_runs_with_models[\"audience_id\"].isin(considered_audiences) == False)]\n",
    "merged_targeting_runs_with_models_null_models = merged_targeting_runs_with_models_null_models[merged_targeting_runs_with_models_null_models[\"tenant\"].isin(ignore_tenants)==False]\n",
    "print(f\"merged_targeting_runs_with_models_null_models = {len(merged_targeting_runs_with_models_null_models)}\")\n",
    "# concate data\n",
    "merged_targeting_runs_with_models = pd.concat([merged_targeting_runs_with_models_with_previous_models, merged_targeting_runs_with_models_without_previous_models,merged_targeting_runs_with_models_null_models])\n",
    "merged_targeting_runs_with_models = merged_targeting_runs_with_models.reset_index(drop=True)\n",
    "print(f\"merged_targeting_runs_with_models = {len(merged_targeting_runs_with_models)}\")\n",
    "targeting_runs_to_compare = targeting_and_retraining_runs[targeting_and_retraining_runs[\"tenant\"].isin(ignore_tenants)==False]\n",
    "if len(merged_targeting_runs_with_models) != len(targeting_runs_to_compare):\n",
    "    raise Exception(f\"Unequal length targeting_runs_to_compare {len(targeting_runs_to_compare)} vs. merged {len(merged_targeting_runs_with_models)}\")\n",
    "merged_targeting_runs_with_models[[\"tenant\",\"audience_id\",\"date\",\"node_name\",\"timestamp\",\"created_data_model_for_testing\",\"targetingOutlookDays\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0410a9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if dataframe does not match lenght - check why\n",
    "vc_before = pd.DataFrame(targeting_and_retraining_runs[targeting_and_retraining_runs[\"tenant\"].isin(ignore_tenants)==False].groupby(by=[\"Prefect_Deployments\",\"audience_id\"])[\"date\"].value_counts()).reset_index().sort_values(by=[\"audience_id\",\"date\"], ascending=False).reset_index(drop=True)\n",
    "vc_after = pd.DataFrame(merged_targeting_runs_with_models.groupby(by=[\"Prefect_Deployments\",\"audience_id\"])[\"date\"].value_counts()).reset_index().sort_values(by=[\"audience_id\",\"date\"], ascending=False).reset_index(drop=True)\n",
    "if vc_after.equals(vc_before):\n",
    "    print(\"fine\")\n",
    "else:\n",
    "    print(\"not fine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768bbc70",
   "metadata": {},
   "source": [
    "# Extract node gb size and cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8789671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_node_parameters(string,node_spec=None):\n",
    "    #print(string)\n",
    "    if isinstance(string, str) == False:\n",
    "        #print(f\"String is not given\")\n",
    "        return None\n",
    "    if \"medium32g\" in string:\n",
    "        if node_spec == \"cpu\":\n",
    "            return None\n",
    "        elif node_spec == \"gb\":\n",
    "            return 32\n",
    "        else:\n",
    "            raise ValueError(f\"Node spec needs to be cpu or gb\")\n",
    "    if \"x2large\" in string:\n",
    "        if node_spec == \"cpu\":\n",
    "            return 29\n",
    "        elif node_spec == \"gb\":\n",
    "            return 350\n",
    "        else:\n",
    "            raise ValueError(f\"Node spec needs to be cpu or gb\")\n",
    "    if \"small\" in string:\n",
    "        if node_spec == \"cpu\":\n",
    "            return 3\n",
    "        elif node_spec == \"gb\":\n",
    "            return 4\n",
    "        else:\n",
    "            raise ValueError(f\"Node spec needs to be cpu or gb\")\n",
    "    if \"xlarge\" in string:\n",
    "        if node_spec == \"cpu\":\n",
    "            return 7\n",
    "        elif node_spec == \"gb\":\n",
    "            return 110\n",
    "        else:\n",
    "            raise ValueError(f\"Node spec needs to be cpu or gb\")\n",
    "    if \"medium64g\" in string:\n",
    "        if node_spec == \"cpu\":\n",
    "            return 3\n",
    "        elif node_spec == \"gb\":\n",
    "            return 55\n",
    "        else:\n",
    "            raise ValueError(f\"Node spec needs to be cpu or gb\")\n",
    "    if \"x4llarge\" in string:\n",
    "        if node_spec == \"cpu\":\n",
    "            return 29\n",
    "        elif node_spec == \"gb\":\n",
    "            return 350\n",
    "        else:\n",
    "            raise ValueError(f\"Node spec needs to be cpu or gb\")\n",
    "    # match the string for cpu and gb\n",
    "    match = re.search(r\"(\\d+)cpu(\\d+)gib\", string)\n",
    "    if match is False:\n",
    "        print(f\"No match found for {string}\")\n",
    "        return None\n",
    "    elif node_spec is None:\n",
    "        raise ValueError(f\"Node spec needs to be cpu or gb\")\n",
    "    elif node_spec == \"cpu\":\n",
    "        return int(match.group(1))\n",
    "    elif node_spec == \"gb\":\n",
    "        return int(match.group(2))\n",
    "    else:\n",
    "        raise ValueError(f\"Node spec needs to be cpu or gb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "557e8acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_targeting_runs_with_models[\"node_gb\"] = merged_targeting_runs_with_models[\"node_name\"].apply(lambda x: extract_node_parameters(x,node_spec=\"gb\"))\n",
    "merged_targeting_runs_with_models[\"node_cpu\"] = merged_targeting_runs_with_models[\"node_name\"].apply(lambda x: extract_node_parameters(x,node_spec=\"cpu\"))\n",
    "merged_targeting_runs_with_models[[\"tenant\",\"audience_id\",\"node_name\",\"timestamp\",\"targetingOutlookDays\",\"node_gb\",\"node_cpu\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ac5aa4",
   "metadata": {},
   "source": [
    "## Add missing node sizes via current node size mapping\n",
    "- extract all known node_names and check if it can be mapped to unknown node_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a2f5339",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_size_mapping_with_size = merged_targeting_runs_with_models.dropna(subset=[\"node_name\"]).drop_duplicates(subset=[\"Prefect_Deployments\",\"audience_id\",\"node_name\",\"targetingOutlookDays\",\"node_gb\",\"node_cpu\"])[[\"Prefect_Deployments\",\"tenant\",\"audience_id\",\"node_name\",\"node_gb\",\"node_cpu\",\"targetingOutlookDays\"]]\n",
    "vc = pd.DataFrame(node_size_mapping_with_size.groupby(\"audience_id\")[\"node_name\"].nunique()).rename(columns={\"node_name\":\"node_count\"})\n",
    "node_size_mapping_with_size = pd.merge(node_size_mapping_with_size, vc, on=\"audience_id\", how=\"left\")\n",
    "vc_max = merged_targeting_runs_with_models.groupby(by=[\"Prefect_Deployments\",\"audience_id\",\"node_name\",\"targetingOutlookDays\",\"node_gb\",\"node_cpu\"])[\"timestamp\"].max().reset_index()\n",
    "node_size_mapping_with_size = pd.merge(node_size_mapping_with_size, vc_max, on=[\"Prefect_Deployments\",\"audience_id\",\"node_name\",\"targetingOutlookDays\",\"node_gb\",\"node_cpu\"], how=\"left\")\n",
    "for col in node_size_mapping_with_size.columns:\n",
    "   node_size_mapping_with_size = node_size_mapping_with_size.rename(columns={col: f\"{col}_nm\"})\n",
    "node_size_mapping_with_size.sort_values(by=[\"tenant_nm\",\"audience_id_nm\",\"Prefect_Deployments_nm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "367fade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_targeting_runs_with_models_with_node_size = pd.merge(\n",
    "    merged_targeting_runs_with_models,\n",
    "    node_size_mapping_with_size,\n",
    "    left_on=[\"Prefect_Deployments\",\"tenant\",\"targetingOutlookDays\"],\n",
    "    right_on = [\"Prefect_Deployments_nm\",\"tenant_nm\",\"targetingOutlookDays_nm\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "merged_targeting_runs_with_models_with_node_size[[\"Prefect_Deployments\",\"tenant\",\"timestamp\",\"audience_id\",\"targetingOutlookDays\",\"node_name\",\"node_gb\",\"node_cpu\"]+node_size_mapping_with_size.columns.tolist()].sort_values(by=[\"tenant\",\"audience_id\",\"Prefect_Deployments\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c136bf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_targeting_runs_with_models_with_node_size = merged_targeting_runs_with_models_with_node_size.sort_values(by=[\"Prefect_Deployments\",\"tenant\",\"audience_id\",\"timestamp\",\"node_gb_nm\"]).drop_duplicates(subset=[\"Prefect_Deployments\",\"tenant\",\"audience_id\",\"timestamp\"],keep=\"last\")\n",
    "merged_targeting_runs_with_models_with_node_size[\"merged_node_name\"] = np.where(\n",
    "    merged_targeting_runs_with_models_with_node_size[\"node_name\"].isnull(),\n",
    "    merged_targeting_runs_with_models_with_node_size[\"node_name_nm\"],\n",
    "    merged_targeting_runs_with_models_with_node_size[\"node_name\"]\n",
    ")\n",
    "merged_targeting_runs_with_models_with_node_size[[\"Prefect_Deployments\",\"tenant\",\"timestamp\",\"audience_id\",\"timestamp\",\"targetingOutlookDays\",\"node_name\",\"node_gb_nm\",\"merged_node_name\"]].sort_values(by=[\"tenant\",\"audience_id\",\"Prefect_Deployments\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e360a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_node_sizes_retraining = read_yaml_from_github(\n",
    "    \"prefect-2-targeting\",\n",
    "    \"config/kubernetes/\",\n",
    "    [\"node_types.yaml\"],\n",
    "    logging\n",
    ")\n",
    "github_node_sizes_retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "934168cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_node_sizes_df = pd.DataFrame.from_dict(github_node_sizes_retraining[\"node_types\"][\"resources\"], orient=\"index\").reset_index().rename(columns={\"index\": \"node_type\"})\n",
    "for col in github_node_sizes_df.columns:\n",
    "    github_node_sizes_df = github_node_sizes_df.rename(columns={col: f\"{col}_github\"})\n",
    "github_node_sizes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7a4358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_targeting_runs_with_models_with_node_size = pd.merge(\n",
    "    merged_targeting_runs_with_models_with_node_size,\n",
    "    github_node_sizes_df,\n",
    "    left_on=\"merged_node_name\",\n",
    "    right_on=\"node_pool_name_github\",\n",
    "    how=\"left\"\n",
    ")\n",
    "merged_targeting_runs_with_models_with_node_size[[\"tenant\",\"timestamp\",\"audience_id\",\"timestamp\",\"targetingOutlookDays\",\"node_name\",\"node_gb_nm\",\"merged_node_name\",\"node_pool_name_github\",\"node_type_github\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76d1efb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_targeting_runs_with_models_with_node_size[\"merged_node_name\"] = np.where(\n",
    "    merged_targeting_runs_with_models_with_node_size[\"merged_node_name\"]==merged_targeting_runs_with_models_with_node_size[\"node_pool_name_github\"],\n",
    "    merged_targeting_runs_with_models_with_node_size[\"node_type_github\"],\n",
    "    merged_targeting_runs_with_models_with_node_size[\"merged_node_name\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4e144d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_targeting_runs_with_models_with_node_size[[\"tenant\",\"timestamp\",\"audience_id\",\"timestamp\",\"targetingOutlookDays\",\"node_name\",\"node_gb_nm\",\"merged_node_name\",\"node_pool_name_github\",\"node_type_github\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0951b9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_targeting_runs_with_models_with_node_size.groupby(by=[\"tenant\",\"Prefect_Deployments\",\"audience_id\"])[\"merged_node_name\"].value_counts().reset_index().sort_values(by=\"audience_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c76cf0",
   "metadata": {},
   "source": [
    "## Check Data Quality of Node Size Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "046385aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_node_size_mapping = merged_targeting_runs_with_models_with_node_size[merged_targeting_runs_with_models_with_node_size[\"merged_node_name\"].isnull()].drop_duplicates(subset=[\"tenant\",\"targetingOutlookDays\"])\n",
    "null_node_size_mapping[[\"tenant\",\"targetingOutlookDays\",\"date\"]]\n",
    "print(f\"Found {len(null_node_size_mapping)} null node size mappings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee2cb79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_node_size_mapping[[\"Prefect_Deployments\",\"tenant\",\"targetingOutlookDays\",\"merged_node_name\",\"audience_id\",\"date\",\"duration\",\"node_name\",\"node_pool_name_github\",\"serviceName\"]]#.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f83485a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_targeting_runs_with_models_with_node_size.to_csv(f\"{path_to_save}extract_targeting_retraining_costs_merged_and_pp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3725472",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_targeting_runs_with_models_with_node_size[[\"tenant\",\"timestamp\",\"audience_id\",\"merged_node_name\",\"duration\",\"charge\",\"part_of_costs\",\"total_charge_of_serviceName\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba35da0",
   "metadata": {},
   "source": [
    "# Extract Valid Time Window for Calculating Costs\n",
    "- erstmal nur StackIT-Kosten berücksichtigen\n",
    "- StackIT 03.11 - 10.11: sehr viele Testruns\n",
    "- stackIT start date: 11.11.2025 (Umzug)\n",
    "- Zeiträume, wo azure lief (erstmal außen vor lassen): 2024-12-22 & 2025-01-03\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "147aaf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackit_cost_handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47815c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = merged_targeting_runs_with_models_with_node_size[merged_targeting_runs_with_models_with_node_size[\"date\"]> stackit_cost_handling[\"start\"]]\n",
    "print(f\"Min to max date in df_cleaned: {df_cleaned['date'].min()} to {df_cleaned['date'].max()}\")\n",
    "for exlcude_dates in stackit_cost_handling[\"exlude_date_ranges\"]:\n",
    "    print(f\"Excluding date range: {exlcude_dates['start']} to {exlcude_dates['end']}\")\n",
    "    idx_to_remove = df_cleaned[\n",
    "        (df_cleaned[\"date\"] >= exlcude_dates[\"start\"]) & (df_cleaned[\"date\"] <= exlcude_dates[\"end\"])\n",
    "    ].index\n",
    "    df_cleaned = df_cleaned.drop(idx_to_remove)\n",
    "df_cleaned = df_cleaned.drop(columns=[\"timestamp_nm\"])\n",
    "df_cleaned=df_cleaned.reset_index(drop=True)\n",
    "df_cleaned[\"date\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e39f71ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned[[\"Prefect_Deployments\",\"tenant\",\"audience_id\",\"date\",\"duration\",\"node_name\",\"merged_node_name\",\"charge\",\"part_of_costs\",\"total_charge_of_serviceName\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4da2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned[\"merged_node_name\"].sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51df0333",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nodes_by_customer = pd.DataFrame(df_cleaned.groupby(by=[\"date\",\"tenant\",\"Prefect_Deployments\"])[\"merged_node_name\"].unique()).reset_index()\n",
    "count_nodes_by_customer.sort_values(by=[\"tenant\",\"date\"], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54b1a297",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_csv(f\"{path_to_save}extract_targeting_retraining_costs_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edbf406",
   "metadata": {},
   "source": [
    "# Get Costs by Service and Date\n",
    "- grouping cost per cloud\n",
    "    - azure: via serviceName & date\n",
    "    - stackit: via serviceName & date\n",
    "    - aws: via serviceName & date\n",
    "    - databricks: via serviceName & date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f37463fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_columns_node_size_costs = [\"cloud\",\"date\",\"node_name\",\"charge\",\"part_of_costs\",\"total_charge_of_serviceName\",\"serviceName\"]\n",
    "data_columns = use_columns_node_size_costs.copy() + [\"serviceCategoryName\",\"quantity\",\"billing_currency\",\"machine.type\"]\n",
    "print(df_orignal[\"date\"].astype(\"string\").min(),df_orignal[\"date\"].astype(\"string\").max())\n",
    "temp_costs = df_orignal.drop_duplicates(subset=use_columns_node_size_costs).reset_index(drop=True)\n",
    "temp_costs = temp_costs[temp_costs[\"date\"]> stackit_cost_handling[\"start\"]]\n",
    "temp_costs = temp_costs.dropna(subset=[\"charge\"])\n",
    "len_data = len(temp_costs)\n",
    "# handle databricks costs\n",
    "temp_costs = stack_pp(temp_costs)\n",
    "temp_costs = handle_aws_costs(temp_costs)\n",
    "temp_costs = handle_azure_costs(temp_costs)\n",
    "temp_costs = handle_databricks_cost(temp_costs)\n",
    "temp_costs = temp_costs[data_columns]\n",
    "# format data\n",
    "temp_costs[\"date\"] = temp_costs[\"date\"].astype(\"string\")\n",
    "temp_costs[\"date\"] = pd.to_datetime(temp_costs[\"date\"], format=\"mixed\", utc=True).dt.strftime(\"%Y-%m-%d\")\n",
    "temp_costs[\"date\"] = temp_costs[\"date\"].astype(\"string\")\n",
    "print(temp_costs[\"date\"].min(),temp_costs[\"date\"].max())\n",
    "if len_data != len(temp_costs): # plus 2 due to agentpools\n",
    "    raise ValueError(\"Length of data does not match\")\n",
    "temp_costs.sort_values(by=[\"date\",\"node_name\",\"serviceName\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9cb4b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum costs over serviceName and same day\n",
    "temp_costs[\"sum_costs_serviceName\"] = temp_costs.groupby(by=[\"date\",\"serviceName\"])[\"charge\"].transform(\"sum\")\n",
    "temp_costs[\"sum_quantity_serviceName\"] = temp_costs.groupby(by=[\"date\",\"serviceName\"])[\"quantity\"].transform(\"sum\")\n",
    "temp_costs = temp_costs.drop_duplicates(subset=[\"cloud\",\"date\",\"serviceName\",\"node_name\",\"sum_costs_serviceName\",\"sum_quantity_serviceName\"])\n",
    "temp_costs = temp_costs[[\"date\",\"node_name\",\"serviceName\",\"sum_costs_serviceName\",\"sum_quantity_serviceName\",\"cloud\",\"serviceCategoryName\"]]\n",
    "temp_costs.sort_values(by=[\"date\",\"cloud\",\"serviceName\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b05915a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_costs[temp_costs[\"cloud\"]==\"aws\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6afa6c2",
   "metadata": {},
   "source": [
    "# Merging Costs and running data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "05c0ce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_costs_node_name_none = temp_costs[temp_costs[\"node_name\"].isna()]\n",
    "temp_costs_not_none = temp_costs[temp_costs[\"node_name\"].notna()]\n",
    "if len(temp_costs_node_name_none) + len(temp_costs_not_none) != len(temp_costs):\n",
    "    raise ValueError(\"Length of data does not match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7640d8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_costs = pd.merge(\n",
    "    df_cleaned,\n",
    "    temp_costs_not_none,\n",
    "    how=\"left\",\n",
    "    left_on=[\"date\",\"merged_node_name\"],\n",
    "    right_on = [\"date\",\"node_name\"],\n",
    "    suffixes=(\"\",\"_costs\")\n",
    ")\n",
    "df_costs[[\"date\",\"Prefect_Deployments\",\"cloud\",\"cloud_costs\",\"account\",\"audience_id\",\"duration\",\"merged_node_name\",\"node_name_costs\",\"serviceName\",\"serviceName_costs\",\"charge\",\"quantity\",\"total_charge_of_serviceName\",'sum_costs_serviceName', 'sum_quantity_serviceName']].sort_values(by=[\"date\",\"merged_node_name\",\"cloud\",\"account\",\"audience_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6c2b429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = df_costs.groupby(by=[\"date\",\"merged_node_name\",\"tenant\",\"audience_id\"])[\"duration\"].value_counts().sort_values(ascending=False)\n",
    "vc = vc[vc>1].reset_index()\n",
    "if vc.empty is False:\n",
    "    raise ValueError(f\"There are duplicates in the data: {vc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f95cbfd",
   "metadata": {},
   "source": [
    "# clean data\n",
    "- fill null cloud values with cloud_costs values\n",
    "- fill null total_charge_of_serviceName costs with sum_costs_serviceName\n",
    "- fill null quantiy with sum_quantity_serviceName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6802ce3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_costs_cleaned = df_costs.copy()\n",
    "# # print(unequal_costs)\n",
    "df_costs_cleaned[\"cloud\"] = np.where(df_costs_cleaned[\"cloud\"].isnull(), df_costs_cleaned[\"cloud_costs\"], df_costs_cleaned[\"cloud\"])\n",
    "df_costs_cleaned[\"total_charge_of_serviceName\"] = np.where(df_costs_cleaned[\"sum_costs_serviceName\"].isnull()==False, df_costs_cleaned[\"sum_costs_serviceName\"], df_costs_cleaned[\"total_charge_of_serviceName\"])\n",
    "df_costs_cleaned[\"serviceName\"] = np.where(df_costs_cleaned[\"serviceName\"].isnull(), df_costs_cleaned[\"serviceName_costs\"], df_costs_cleaned[\"serviceName\"])\n",
    "df_costs_cleaned[\"quantity\"] = np.where(df_costs_cleaned[\"sum_quantity_serviceName\"].isnull()==False, df_costs_cleaned[\"sum_quantity_serviceName\"], df_costs_cleaned[\"quantity\"])\n",
    "df_costs_cleaned[\"sum_duration_serviceName\"] = df_costs_cleaned.groupby(by=[\"date\",\"serviceName\"])[\"duration\"].transform(\"sum\")\n",
    "df_costs_cleaned[\"sum_duration_serviceName\"] = np.where(\n",
    "    df_costs_cleaned[\"serviceName\"].str.contains(\"Purpose Server-g1.3-EU01\").fillna(False),\n",
    "    df_costs_cleaned[\"sum_duration_serviceName\"]+(2*86400), #add agent pools duration\n",
    "    df_costs_cleaned[\"sum_duration_serviceName\"]\n",
    ")\n",
    "df_costs_cleaned = df_costs_cleaned.drop(columns=[\"cloud_costs\",\"serviceName_costs\",\"sum_costs_serviceName\",\"sum_quantity_serviceName\",\"charge\"])\n",
    "df_costs_cleaned[[\"date\",\"Prefect_Deployments\",\"cloud\",\"account\",\"audience_id\",\"duration\",\"merged_node_name\",\"node_name_costs\",\"serviceName\",\"quantity\",\"total_charge_of_serviceName\",\"serviceCategoryName\"]].sort_values(by=[\"date\",\"merged_node_name\",\"cloud\",\"account\",\"audience_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6b80812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO check for resetting the model\n",
    "accounts = call_api_with_service_token(f\"{url}/core/accounts/query\", {}, logging)\n",
    "null_charges = df_costs_cleaned[df_costs_cleaned[\"total_charge_of_serviceName\"].isnull()]\n",
    "null_charges.shape\n",
    "if len(null_charges) > 20:\n",
    "    raise ValueError(f\"Found more than {len(null_charges)} null charges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fa39021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_costs_cleaned.to_csv(f\"{path_to_save}extract_targeting_retraining_costs_final_{from_date}_{to_date}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7f82adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_costs_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "784464fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# welche kosten gehören noch zum targeing, die nicht die node sizes betreffen?\n",
    "#TODO: missing azure node_names .. ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80b2b3e",
   "metadata": {},
   "source": [
    "# Check samples of data for a certain date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "830dde58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for testing\n",
    "def return_deployment_counts(test_data: pd.DataFrame):\n",
    "    return test_data[\"Prefect_Deployments\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff65193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_cumulated_costs(test_data: pd.DataFrame, return_part_of_costs_per_run=False):\n",
    "    cum =  test_data.groupby(\"serviceName\")[\"cost_per_run\"].sum().sort_values(ascending=False)\n",
    "    cum = pd.DataFrame(cum).reset_index()\n",
    "    if return_part_of_costs_per_run is True:\n",
    "        cum_part = test_data.groupby(\"serviceName\")[\"part_of_costs_per_run\"].sum().sort_values(ascending=False)\n",
    "        cum_part = pd.DataFrame(cum_part).reset_index()\n",
    "        cum = pd.merge(cum, cum_part, on=\"serviceName\")\n",
    "    return cum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c637178",
   "metadata": {},
   "source": [
    "## 10.06.2025\n",
    "Prefect Flows:\n",
    "- expected retrainings: 6\n",
    "- expected targetings: 114\n",
    "    - total: 119 - 1 - 4 = 114\n",
    "    - 1 wurde nach wenigen sekunden gecancelt\n",
    "    - 4: kein pod wurde gestartet - also keine logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2ffdd309",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_columns = [\"date\",\"timestamp\",\"Prefect_Deployments\",\"cloud\",\"account\",\"audience_id\",\"duration\",\"sum_duration_serviceName\",\"merged_node_name\",\"node_name_costs\",\"serviceName\",\"quantity\",\"total_charge_of_serviceName\"]\n",
    "date_20250610 = df_costs_cleaned[df_costs_cleaned[\"date\"] == \"2025-06-10\"][use_columns]\n",
    "expected_runs = {\"targeting\": 114, \"retraining\": 6, \"googleConversionUpdate\": 5}\n",
    "expected_costs_dictionary = {\n",
    "    \"header\": [\"Service\", \"SKU\", \"Gesamtmenge\", \"Gesamtkosten\"],\n",
    "    \"date\": \"20250610\",\n",
    "    \"data\": [\n",
    "        {\"Service\": \"Memory Optimized Server-b1.4-EU01\", \"SKU\": \"ST-0020701\", \"Gesamtmenge\": \"17 Hours\", \"Gesamtkosten\": \"13.73\"},\n",
    "        {\"Service\": \"General Purpose Server-g1.3-EU01\", \"SKU\": \"ST-0006901\", \"Gesamtmenge\": \"87 Hours\", \"Gesamtkosten\": \"13.19\"},\n",
    "        {\"Service\": \"Memory Optimized Server-m1.3-EU01\", \"SKU\": \"ST-0009301\", \"Gesamtmenge\": \"44 Hours\", \"Gesamtkosten\": \"8.47\"},\n",
    "        {\"Service\": \"General Purpose Server-g1.4-EU01\", \"SKU\": \"ST-0007301\", \"Gesamtmenge\": \"24 Hours\", \"Gesamtkosten\": \"7.28\"},\n",
    "        {\"Service\": \"Memory Optimized Server-b1.3-EU01\", \"SKU\": \"ST-0020501\", \"Gesamtmenge\": \"17 Hours\", \"Gesamtkosten\": \"6.86\"},\n",
    "        {\"Service\": \"Compute Optimized Server-c1.3-EU01\", \"SKU\": \"ST-0008901\", \"Gesamtmenge\": \"43 Hours\", \"Gesamtkosten\": \"5.93\"},\n",
    "        {\"Service\": \"Block Storage for disk volumes Premium-Performance 2-EU01\", \"SKU\": \"ST-0011001\", \"Gesamtmenge\": \"244 Hours\", \"Gesamtkosten\": \"4.91\"},\n",
    "        {\"Service\": \"Memory Optimized Server-b1a.16d-EU01\", \"SKU\": \"ST-0031001\", \"Gesamtmenge\": \"2 Hours\", \"Gesamtkosten\": \"4.19\"},\n",
    "        {\"Service\": \"PostgreSQL-Flex-2.4-Single-EU01\", \"SKU\": \"ST-0025701\", \"Gesamtmenge\": \"24 Hours\", \"Gesamtkosten\": \"3.02\"},\n",
    "        {\"Service\": \"Essential-Network-Load-Balancer-10-EU01\", \"SKU\": \"ST-0062401\", \"Gesamtmenge\": \"211 Hours\", \"Gesamtkosten\": \"2.75\"},\n",
    "        {\"Service\": \"Kubernetes Engine-Cluster Management-EU01\", \"SKU\": \"ST-0010701\", \"Gesamtmenge\": \"24 Hours\", \"Gesamtkosten\": \"2.39\"},\n",
    "        {\"Service\": \"Tiny Server-t1.2-EU01\", \"SKU\": \"ST-0009101\", \"Gesamtmenge\": \"390 Hours\", \"Gesamtkosten\": \"2.13\"},\n",
    "        {\"Service\": \"Compute Optimized Server-c1.2-EU01\", \"SKU\": \"ST-0008301\", \"Gesamtmenge\": \"33 Hours\", \"Gesamtkosten\": \"2.00\"},\n",
    "        {\"Service\": \"General Purpose Server-g1.1-EU01\", \"SKU\": \"ST-0008501\", \"Gesamtmenge\": \"48 Hours\", \"Gesamtkosten\": \"1.82\"},\n",
    "        {\"Service\": \"Block Storage for disk volumes Premium-Capacity-EU01\", \"SKU\": \"ST-0018001\", \"Gesamtmenge\": \"11.598 Gigabyte Hours\", \"Gesamtkosten\": \"1.05\"},\n",
    "        {\"Service\": \"Public IP Address (IPv4)-EU01\", \"SKU\": \"ST-0070001\", \"Gesamtmenge\": \"219 Hours\", \"Gesamtkosten\": \"0.89\"},\n",
    "        {\"Service\": \"Block Storage for disk volumes Premium-Performance 1-EU01\", \"SKU\": \"ST-0010901\", \"Gesamtmenge\": \"72 Hours\", \"Gesamtkosten\": \"0.73\"},\n",
    "        {\"Service\": \"Block Storage for PostgreSQL Premium-Performance 2-EU01\", \"SKU\": \"ST-0068501\", \"Gesamtmenge\": \"24 Hours\", \"Gesamtkosten\": \"0.48\"},\n",
    "        {\"Service\": \"Block Storage for PostgreSQL Premium-Capacity-EU01\", \"SKU\": \"ST-0069101\", \"Gesamtmenge\": \"1.200 Gigabyte Hours\", \"Gesamtkosten\": \"0.11\"},\n",
    "        {\"Service\": \"Backup Storage for PostgreSQL Premium-EU01\", \"SKU\": \"ST-0040001\", \"Gesamtmenge\": \"2.455 Gigabyte Hours\", \"Gesamtkosten\": \"0.09\"},\n",
    "        {\"Service\": \"Observability-Metrics-Endpoint-100k-EU01\", \"SKU\": \"ST-0101401\", \"Gesamtmenge\": \"24 Hours\", \"Gesamtkosten\": \"0.00\"},\n",
    "        ]\n",
    "    }\n",
    "expected_costs_dictionary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a621cff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_20250610 = return_deployment_counts(date_20250610)\n",
    "for key in expected_runs.keys():\n",
    "    if key not in vc_20250610.index:\n",
    "        raise ValueError(f\"Key {key} not found in {vc_20250610.index}\")\n",
    "    if vc_20250610[key] != expected_runs[key]:\n",
    "        raise ValueError(f\"Expected value {expected_runs[key]} for {key}, but got {vc_20250610[key]}\")\n",
    "vc_20250610"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "76f2f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_20250610 = return_cost_per_run(date_20250610)\n",
    "date_20250610.sort_values(by=[\"serviceName\",\"account\",\"audience_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a1ce6411",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulated_costs = return_cumulated_costs(date_20250610, return_part_of_costs_per_run=True)\n",
    "cumulated_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f19130e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_20250610 = pd.DataFrame(columns=[\"serviceName\", \"cost_per_run\", \"expected_costs\", \"diff\", \"diff_in_perc\"])\n",
    "for cost_entry in expected_costs_dictionary[\"data\"]:\n",
    "    serviceName = cost_entry[\"Service\"]\n",
    "    #if \"Server\" not in serviceName:\n",
    "    #    continue\n",
    "    expected_costs = cost_entry[\"Gesamtkosten\"]\n",
    "    expected_costs = float(expected_costs)\n",
    "    print(serviceName, expected_costs)\n",
    "    cumulated_costs_temp = cumulated_costs[cumulated_costs[\"serviceName\"] == serviceName]\n",
    "    if len(cumulated_costs_temp) != 1:\n",
    "       print(f\"Found {len(cumulated_costs_temp)} entries for serviceName {serviceName}\")\n",
    "       cumulated_costs_temp_value = None\n",
    "    else:\n",
    "        cumulated_costs_temp_value = cumulated_costs_temp[\"cost_per_run\"].round(2).values[0]\n",
    "        if serviceName == \"General Purpose Server-g1.3-EU01\":\n",
    "            #special case for agentpools\n",
    "            known_part = cumulated_costs_temp[\"part_of_costs_per_run\"].values[0]\n",
    "            total_costs = np.round(cumulated_costs_temp_value / known_part,2)\n",
    "            print(f\"  total costs: {total_costs}\")\n",
    "            print(f\"  missing part for agentpools: {total_costs-cumulated_costs_temp_value}\")\n",
    "            cumulated_costs_temp_value = total_costs\n",
    "    if cumulated_costs_temp_value != expected_costs:\n",
    "        print(f\"Expected {expected_costs} for serviceName {serviceName}, but got {cumulated_costs_temp_value}\")\n",
    "    if cumulated_costs_temp_value is None:\n",
    "        diff = None\n",
    "        diff_in_perc = None\n",
    "    else:\n",
    "        diff = cumulated_costs_temp_value - expected_costs if cumulated_costs is not None else None\n",
    "        diff_in_perc = (cumulated_costs_temp_value - expected_costs) / expected_costs\n",
    "    res_20250610.loc[len(res_20250610)] = [\n",
    "        serviceName,\n",
    "        cumulated_costs_temp_value,\n",
    "        expected_costs,\n",
    "        diff,\n",
    "        diff_in_perc\n",
    "    ]\n",
    "res_20250610"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482f7bc9",
   "metadata": {},
   "source": [
    "# OPEN QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371be7d",
   "metadata": {},
   "source": [
    "General Purpose Server-g1.4-EU01 (8 CPU, 32 GB RAM)\n",
    "- nicht im k8s node pool\n",
    "- innkeepr-analytics-2?\n",
    "\n",
    "Tiny Server-t1.2-EU01 (1 CPU, 1 RAM)\n",
    "- nicht im k8s node pool\n",
    "- k8s-svc-7e52454b-48a0-4719-b97c-0d94ca2fb517-ga-conversion-upda -> läuft der, wenn die die core/queue bearbeitet wird?\n",
    "\n",
    "Compute Optimized Server-c1.2-EU01 (2 CPU, 4 GB RAM)\n",
    "- k8s: 2cpu4gib\n",
    "- postgres-prefect (2 CPU, 4GB)\n",
    "\n",
    "General Purpose Server-g1.1-EU01 (1 CPU, 4 RAM)\n",
    "- nicht im k8s node pool\n",
    "- analytics-testing-server\n",
    "\n",
    "k8s agentpool:\n",
    "  - agentpool: General Purpose Server-g1.3-EU01 \n",
    "  - Den sehe ich zweimal in den Servern - für die Region eu01-3 und eu01-2. Laufen die beide 24h?\n",
    "  - #TODO: Muss herausgerechnet werden / läuft 24h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c7f461",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "consumption-based-costs-wGvc2ut4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
